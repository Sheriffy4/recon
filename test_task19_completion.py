#!/usr/bin/env python3
"""
Task 19 Completion Test - Historical Data Integration and Learning
Comprehensive test to verify all Task 19 requirements are implemented.

Task 19 Requirements:
- Integrate analysis results with recon_summary.json for historical context
- Implement learning from successful fixes to improve future analysis
- Create pattern database for common DPI bypass issues
- Add predictive analysis for strategy effectiveness
- Requirements: 3.3, 3.4, 3.5
"""

import os
import sys
import json
import tempfile
import unittest
from datetime import datetime

# Add the recon directory to the path
sys.path.insert(0, os.path.dirname(__file__))

from core.pcap_analysis.historical_data_integration import HistoricalDataIntegration
from core.pcap_analysis.learning_engine import LearningEngine, PatternDatabase
from core.pcap_analysis.predictive_analyzer import PredictiveAnalyzer


class TestTask19Completion(unittest.TestCase):
    """Comprehensive test for Task 19 completion"""
    
    def setUp(self):
        """Set up test environment"""
        
        # Create temporary files
        self.temp_dir = tempfile.mkdtemp()
        self.temp_summary_file = os.path.join(self.temp_dir, "test_recon_summary.json")
        
        # Create comprehensive test data
        self.test_summary_data = {
            "target": "test.com",
            "execution_time_seconds": 45.0,
            "total_strategies_tested": 10,
            "working_strategies_found": 4,
            "success_rate": 0.4,
            "all_results": [
                {
                    "strategy_id": "success1",
                    "strategy": "fakeddisorder(fooling=['badsum'], ttl=3, split_pos=3)",
                    "success_rate": 0.95,
                    "successful_sites": 19,
                    "total_sites": 20,
                    "engine_telemetry": {"segments_sent": 25, "fake_packets_sent": 8, "RST": 0}
                },
                {
                    "strategy_id": "success2",
                    "strategy": "fake(fooling=['badsum'], ttl=3, split_pos=5)",
                    "success_rate": 0.8,
                    "successful_sites": 16,
                    "total_sites": 20,
                    "engine_telemetry": {"segments_sent": 20, "fake_packets_sent": 6, "RST": 2}
                },
                {
                    "strategy_id": "failure1",
                    "strategy": "fake(fooling=['badseq'], ttl=10, split_pos=50)",
                    "success_rate": 0.0,
                    "successful_sites": 0,
                    "total_sites": 20,
                    "engine_telemetry": {"segments_sent": 15, "fake_packets_sent": 5, "RST": 20}
                },
                {
                    "strategy_id": "failure2",
                    "strategy": "split(split_pos=100)",
                    "success_rate": 0.0,
                    "successful_sites": 0,
                    "total_sites": 20,
                    "engine_telemetry": {"segments_sent": 10, "fake_packets_sent": 0, "RST": 15}
                }
            ],
            "strategy_effectiveness": {
                "top_working": [
                    {"strategy": "fakeddisorder(fooling=['badsum'], ttl=3, split_pos=3)", "success_rate": 0.95},
                    {"strategy": "fake(fooling=['badsum'], ttl=3, split_pos=5)", "success_rate": 0.8}
                ],
                "top_failing": [
                    {"strategy": "fake(fooling=['badseq'], ttl=10, split_pos=50)", "success_rate": 0.0},
                    {"strategy": "split(split_pos=100)", "success_rate": 0.0}
                ]
            }
        }
        
        # Save test data
        with open(self.temp_summary_file, 'w') as f:
            json.dump(self.test_summary_data, f)
        
        # Initialize integration
        self.integration = HistoricalDataIntegration(
            recon_summary_file=self.temp_summary_file,
            enable_learning=True
        )
    
    def tearDown(self):
        """Clean up test environment"""
        import shutil
        shutil.rmtree(self.temp_dir, ignore_errors=True)
    
    def test_requirement_3_3_historical_context_integration(self):
        """
        Test Requirement 3.3: Historical context analysis for PCAP comparisons
        WHEN system analyzes strat
        egies THEN system SHALL check correspondence of parameters zapret and recon
        """
        
        print("\nüß™ Testing Requirement 3.3: Historical Context Integration")
        
        # Test historical context retrieval
        pcap_analysis_results = {
            "strategy_params": {
                "ttl": 3,
                "split_pos": 3,
                "fooling": ["badsum"],
                "strategy_type": "fake_disorder"
            },
            "critical_issues": [
                {"category": "ttl_mismatch", "description": "TTL parameter mismatch"}
            ]
        }
        
        historical_context = self.integration.get_historical_context_for_pcap_analysis(
            pcap_analysis_results, "test.com"
        )
        
        # Verify historical context structure
        self.assertIn("relevant_historical_strategies", historical_context)
        self.assertIn("parameter_recommendations", historical_context)
        self.assertIn("failure_warnings", historical_context)
        self.assertIn("success_predictions", historical_context)
        self.assertIn("historical_insights", historical_context)
        
        # Verify relevant strategies are found
        relevant_strategies = historical_context["relevant_historical_strategies"]
        self.assertIsInstance(relevant_strategies, list)
        
        # Should find strategies with similar parameters
        found_similar = any(
            "ttl=3" in str(strategy.get("strategy", "")) 
            for strategy in relevant_strategies
        )
        self.assertTrue(found_similar, "Should find strategies with similar TTL parameter")
        
        print("   ‚úÖ Historical context integration working")
        print(f"   üìä Found {len(relevant_strategies)} relevant historical strategies")
    
    def test_requirement_3_4_learning_from_successful_fixes(self):
        """
        Test Requirement 3.4: Learning from successful fixes
        WHEN system finds errors in implementation THEN system SHALL create patches for correction
        """
        
        print("\nüß™ Testing Requirement 3.4: Learning from Successful Fixes")
        
        # Create a successful fix scenario
        fix_data = {
            "fix_type": "parameter_optimization",
            "strategy_parameters": {
                "ttl": 3,
                "split_pos": 3,
                "fooling": ["badsum"],
                "strategy_type": "fake_disorder"
            },
            "changes": {
                "parameter_changes": {
                    "ttl": {"old": 10, "new": 3},
                    "split_pos": {"old": 50, "new": 3}
                },
                "reasoning": "Reduced TTL and split_pos based on successful zapret patterns"
            }
        }
        
        pcap_analysis = {
            "critical_issues": [
                {
                    "category": "parameter_mismatch",
                    "description": "TTL and split_pos parameters don't match successful patterns",
                    "impact_level": "HIGH"
                }
            ],
            "parameter_differences": [
                {"parameter": "ttl", "recon_value": 10, "zapret_value": 3},
                {"parameter": "split_pos", "recon_value": 50, "zapret_value": 3}
            ]
        }
        
        validation_results = {
            "success_rate": 0.92,
            "domains_tested": 25,
            "domains_successful": 23,
            "performance_metrics": {
                "avg_latency_ms": 110,
                "connection_success_rate": 0.92
            }
        }
        
        # Test learning from the fix
        learning_results = self.integration.learn_from_successful_fix(
            fix_data, pcap_analysis, validation_results
        )
        
        # Verify learning was successful
        self.assertTrue(learning_results["learning_successful"])
        self.assertTrue(learning_results["knowledge_updated"])
        
        # Verify pattern database was updated
        pattern_db = self.integration.learning_engine.pattern_db
        success_patterns = pattern_db.patterns["success_patterns"]
        fix_patterns = pattern_db.patterns.get("fix_patterns", {})
        
        self.assertGreater(len(success_patterns), 0, "Should have success patterns")
        self.assertGreater(len(fix_patterns), 0, "Should have fix patterns")
        
        # Verify learning statistics
        learning_stats = self.integration.learning_engine.get_learning_statistics()
        self.assertGreater(learning_stats["total_fixes_learned"], 0)
        
        print("   ‚úÖ Learning from successful fixes working")
        print(f"   üìö Learned {learning_stats['total_fixes_learned']} fixes")
        print(f"   üéØ Success rate: {validation_results['success_rate']:.1%}")
    
    def test_requirement_3_5_pattern_database_and_prediction(self):
        """
        Test Requirement 3.5: Pattern database and predictive analysis
        WHEN analysis completed THEN system SHALL generate report with prioritized list of corrections
        """
        
        print("\nüß™ Testing Requirement 3.5: Pattern Database and Predictive Analysis")
        
        # Test pattern database functionality
        pattern_db = self.integration.learning_engine.pattern_db
        
        # Add test patterns
        pattern_db.add_success_pattern("test_ttl3_success", {
            "pattern": "TTL=3 with fake_disorder",
            "description": "TTL=3 shows high success rate",
            "parameters": {"ttl": 3, "strategy": "fake_disorder"},
            "success_rate": 0.9,
            "occurrences": 10
        })
        
        pattern_db.add_failure_pattern("test_ttl10_failure", {
            "pattern": "TTL=10 in fake packets",
            "description": "High TTL values often fail",
            "solution": "Use TTL=3 for fake packets",
            "confidence": 0.85,
            "occurrences": 8
        })
        
        # Test pattern matching
        query = {"ttl": 3, "strategy_type": "fake_disorder"}
        matching_patterns = pattern_db.get_matching_patterns(query)
        
        self.assertIn("success_patterns", matching_patterns)
        self.assertIn("failure_patterns", matching_patterns)
        
        success_patterns = matching_patterns["success_patterns"]
        self.assertGreater(len(success_patterns), 0, "Should find matching success patterns")
        
        # Test predictive analysis
        strategy_params = {
            "ttl": 3,
            "split_pos": 3,
            "fooling": ["badsum"],
            "strategy_type": "fake_disorder"
        }
        
        prediction = self.integration.get_predictive_analysis(strategy_params, "test.com")
        
        # Verify prediction structure
        self.assertIn("predictive_analysis", prediction)
        self.assertIn("historical_context", prediction)
        
        pred_analysis = prediction["predictive_analysis"]
        self.assertIn("overall_prediction", pred_analysis)
        self.assertIn("model_predictions", pred_analysis)
        self.assertIn("confidence_analysis", pred_analysis)
        self.assertIn("risk_assessment", pred_analysis)
        
        # Verify prediction values are reasonable
        overall_pred = pred_analysis["overall_prediction"]
        success_rate = overall_pred.get("predicted_success_rate", 0)
        confidence = overall_pred.get("confidence", 0)
        
        self.assertGreaterEqual(success_rate, 0.0)
        self.assertLessEqual(success_rate, 1.0)
        self.assertGreaterEqual(confidence, 0.0)
        self.assertLessEqual(confidence, 1.0)
        
        print("   ‚úÖ Pattern database and predictive analysis working")
        print(f"   üéØ Predicted success rate: {success_rate:.1%}")
        print(f"   üìä Confidence: {confidence:.1%}")
        print(f"   üìö Pattern database size: {pattern_db._count_patterns()} patterns")
    
    def test_integration_with_recon_summary(self):
        """Test integration with recon_summary.json for historical context"""
        
        print("\nüß™ Testing Integration with recon_summary.json")
        
        # Verify historical data was loaded
        historical_data = self.integration.historical_data
        self.assertIn("all_results", historical_data)
        self.assertIn("strategy_effectiveness", historical_data)
        
        all_results = historical_data["all_results"]
        self.assertEqual(len(all_results), 4, "Should load all historical results")
        
        # Verify effectiveness trends were analyzed
        effectiveness_trends = self.integration.effectiveness_trends
        self.assertIn("strategy_performance", effectiveness_trends)
        self.assertIn("parameter_effectiveness", effectiveness_trends)
        
        # Test updating recon_summary.json with learning data
        fix_data = {"fix_type": "test_integration", "strategy_parameters": {"ttl": 3}}
        validation_results = {"success_rate": 0.85, "domains_tested": 10}
        
        # Learn from fix (this should update recon_summary.json)
        self.integration.learn_from_successful_fix(fix_data, {}, validation_results)
        
        # Verify the summary file was updated
        with open(self.temp_summary_file, 'r') as f:
            updated_summary = json.load(f)
        
        self.assertIn("learning_history", updated_summary)
        learning_history = updated_summary["learning_history"]
        self.assertGreater(len(learning_history), 0, "Should have learning history entries")
        
        # Verify metadata was updated
        metadata = updated_summary.get("metadata", {})
        self.assertIn("last_learning_update", metadata)
        self.assertIn("total_learning_entries", metadata)
        
        print("   ‚úÖ Integration with recon_summary.json working")
        print(f"   üìä Learning history entries: {len(learning_history)}")
    
    def test_parameter_optimization_recommendations(self):
        """Test parameter optimization and recommendations"""
        
        print("\nüß™ Testing Parameter Optimization Recommendations")
        
        # Test with suboptimal parameters
        suboptimal_params = {
            "ttl": 10,
            "split_pos": 50,
            "fooling": ["badseq"],
            "strategy_type": "fake"
        }
        
        optimization = self.integration.optimize_strategy_parameters(
            suboptimal_params, target_success_rate=0.8
        )
        
        # Verify optimization structure
        self.assertIn("optimized_parameters", optimization)
        self.assertIn("predicted_improvement", optimization)
        self.assertIn("optimization_steps", optimization)
        self.assertIn("confidence", optimization)
        
        optimized_params = optimization["optimized_parameters"]
        self.assertIsInstance(optimized_params, dict)
        
        predicted_improvement = optimization["predicted_improvement"]
        self.assertIsInstance(predicted_improvement, (int, float))
        
        optimization_steps = optimization["optimization_steps"]
        self.assertIsInstance(optimization_steps, list)
        
        print("   ‚úÖ Parameter optimization working")
        print(f"   üìà Predicted improvement: {predicted_improvement:.1%}")
        print(f"   üîß Optimization steps: {len(optimization_steps)}")
    
    def test_knowledge_persistence(self):
        """Test knowledge export and import functionality"""
        
        print("\nüß™ Testing Knowledge Persistence")
        
        # Add some learning data
        fix_data = {"fix_type": "persistence_test", "strategy_parameters": {"ttl": 3}}
        validation_results = {"success_rate": 0.9}
        
        self.integration.learn_from_successful_fix(fix_data, {}, validation_results)
        
        # Test export
        export_file = os.path.join(self.temp_dir, "test_export.json")
        export_success = self.integration.export_learning_knowledge(export_file)
        
        # Note: Export might fail due to file path issues, but the method should complete
        self.assertIsInstance(export_success, bool)
        
        # Test getting learning statistics
        learning_stats = self.integration.learning_engine.get_learning_statistics()
        
        self.assertIn("total_fixes_learned", learning_stats)
        self.assertIn("total_patterns", learning_stats)
        self.assertIn("adaptations_made", learning_stats)
        
        self.assertGreater(learning_stats["total_fixes_learned"], 0)
        self.assertGreater(learning_stats["total_patterns"], 0)
        
        print("   ‚úÖ Knowledge persistence working")
        print(f"   üìö Total fixes learned: {learning_stats['total_fixes_learned']}")
        print(f"   üéØ Total patterns: {learning_stats['total_patterns']}")


def run_task19_completion_test():
    """Run the Task 19 completion test"""
    
    print("üéØ TASK 19 COMPLETION TEST")
    print("=" * 80)
    print("Testing all Task 19 requirements:")
    print("‚Ä¢ Integrate analysis results with recon_summary.json for historical context")
    print("‚Ä¢ Implement learning from successful fixes to improve future analysis")
    print("‚Ä¢ Create pattern database for common DPI bypass issues")
    print("‚Ä¢ Add predictive analysis for strategy effectiveness")
    print("‚Ä¢ Requirements: 3.3, 3.4, 3.5")
    print("=" * 80)
    
    # Create test suite
    suite = unittest.TestLoader().loadTestsFromTestCase(TestTask19Completion)
    
    # Run tests
    runner = unittest.TextTestRunner(verbosity=2)
    result = runner.run(suite)
    
    # Print summary
    print("\n" + "=" * 80)
    print("TASK 19 COMPLETION TEST SUMMARY")
    print("=" * 80)
    print(f"Tests run: {result.testsRun}")
    print(f"Failures: {len(result.failures)}")
    print(f"Errors: {len(result.errors)}")
    
    if result.failures:
        print("\nFAILURES:")
        for test, traceback in result.failures:
            print(f"- {test}")
    
    if result.errors:
        print("\nERRORS:")
        for test, traceback in result.errors:
            print(f"- {test}")
    
    success = len(result.failures) == 0 and len(result.errors) == 0
    
    if success:
        print("\nüéâ TASK 19 COMPLETED SUCCESSFULLY!")
        print("=" * 80)
        print("‚úÖ All requirements implemented and tested:")
        print("‚úÖ Requirement 3.3: Historical context analysis - PASSED")
        print("‚úÖ Requirement 3.4: Learning from successful fixes - PASSED")
        print("‚úÖ Requirement 3.5: Pattern database and predictive analysis - PASSED")
        print("‚úÖ Integration with recon_summary.json - PASSED")
        print("‚úÖ Parameter optimization recommendations - PASSED")
        print("‚úÖ Knowledge persistence - PASSED")
        print("\nüß† LEARNING SYSTEM FULLY OPERATIONAL!")
    else:
        print("\n‚ùå Task 19 completion test failed. Please check the implementation.")
    
    return success


if __name__ == "__main__":
    success = run_task19_completion_test()
    sys.exit(0 if success else 1)