#!/usr/bin/env python3
"""
–í–∞–ª–∏–¥–∞—Ç–æ—Ä –ø–∞—Ä–∏—Ç–µ—Ç–∞ –∞—Ç–∞–∫ –º–µ–∂–¥—É discovery –∏ service —Ä–µ–∂–∏–º–∞–º–∏
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç –ø–æ–ª–Ω—É—é —Å–∏—Å—Ç–µ–º—É –∞–Ω–∞–ª–∏–∑–∞ –ª–æ–≥–æ–≤ –∏ PCAP
"""

import os
import json
import sys
from pathlib import Path
from datetime import datetime
from core.attack_parity.analyzer import AttackParityAnalyzer
from core.attack_parity.report_generator import AttackParityReportGenerator

class AttackParityValidator:
    """–í–∞–ª–∏–¥–∞—Ç–æ—Ä –ø–∞—Ä–∏—Ç–µ—Ç–∞ –∞—Ç–∞–∫"""
    
    def __init__(self):
        self.analyzer = AttackParityAnalyzer(timing_tolerance=0.1)
        self.report_generator = AttackParityReportGenerator()
        Path("reports").mkdir(exist_ok=True)
    
    def validate_parity_with_pcap(self, domain, discovery_log, service_log, discovery_pcap, service_pcap):
        """–ü–æ–ª–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è –ø–∞—Ä–∏—Ç–µ—Ç–∞ —Å PCAP —Ñ–∞–π–ª–∞–º–∏"""
        
        print(f"\nüîç –í–ê–õ–ò–î–ê–¶–ò–Ø –ü–ê–†–ò–¢–ï–¢–ê: {domain}")
        print("=" * 50)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ñ–∞–π–ª–æ–≤
        files_to_check = {
            'Discovery Log': discovery_log,
            'Service Log': service_log,
            'Discovery PCAP': discovery_pcap,
            'Service PCAP': service_pcap
        }
        
        missing_files = []
        for name, file_path in files_to_check.items():
            if not os.path.exists(file_path):
                missing_files.append(f"{name}: {file_path}")
            else:
                size = os.path.getsize(file_path)
                print(f"‚úÖ {name}: {size} –±–∞–π—Ç")
        
        if missing_files:
            print(f"\n‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ —Ñ–∞–π–ª—ã:")
            for missing in missing_files:
                print(f"   {missing}")
            return None
        
        try:
            print(f"\nüìä –ó–∞–ø—É—Å–∫ –∞–Ω–∞–ª–∏–∑–∞ –ø–∞—Ä–∏—Ç–µ—Ç–∞...")
            
            # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø–∞—Ä–∏—Ç–µ—Ç–∞
            result = self.analyzer.analyze_parity(
                discovery_log_path=discovery_log,
                service_log_path=service_log,
                discovery_pcap_path=discovery_pcap,
                service_pcap_path=service_pcap
            )
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç—á–µ—Ç
            report = self.report_generator.generate_comprehensive_report(result)
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            analysis = self.analyze_parity_results(result)
            
            print(f"\nüìà –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ê–ù–ê–õ–ò–ó–ê:")
            print(f"Semantic Accuracy: {result.semantic_accuracy:.2%}")
            print(f"Truth Consistency: {result.truth_consistency_score:.2%}")
            print(f"Parity Score: {result.parity_score:.2%}")
            print(f"Timing Alignment: {analysis['timing_quality']}")
            
            # –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑
            if hasattr(result, 'discrepancies') and result.discrepancies:
                print(f"\n‚ö†Ô∏è  –û–ë–ù–ê–†–£–ñ–ï–ù–ù–´–ï –†–ê–°–•–û–ñ–î–ï–ù–ò–Ø ({len(result.discrepancies)}):")
                for i, discrepancy in enumerate(result.discrepancies[:5], 1):
                    print(f"  {i}. {discrepancy.description}")
                if len(result.discrepancies) > 5:
                    print(f"  ... –∏ –µ—â–µ {len(result.discrepancies) - 5}")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            result_file = f"reports/parity_validation_{domain}_{timestamp}.json"
            
            result_data = {
                'domain': domain,
                'timestamp': timestamp,
                'files': files_to_check,
                'metrics': {
                    'semantic_accuracy': result.semantic_accuracy,
                    'truth_consistency': result.truth_consistency_score,
                    'parity_score': result.parity_score
                },
                'analysis': analysis,
                'report': report,
                'discrepancies': [
                    {
                        'description': d.description,
                        'severity': getattr(d, 'severity', 'unknown'),
                        'category': getattr(d, 'category', 'unknown')
                    } for d in (result.discrepancies if hasattr(result, 'discrepancies') else [])
                ]
            }
            
            with open(result_file, 'w', encoding='utf-8') as f:
                json.dump(result_data, f, indent=2, ensure_ascii=False)
            
            print(f"\nüíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {result_file}")
            
            return result_data
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {e}")
            return None
    
    def analyze_parity_results(self, result):
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–∞—Ä–∏—Ç–µ—Ç–∞"""
        analysis = {
            'overall_quality': 'unknown',
            'timing_quality': 'unknown',
            'semantic_quality': 'unknown',
            'recommendations': []
        }
        
        # –û—Ü–µ–Ω–∫–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞
        if result.semantic_accuracy >= 0.95:
            analysis['semantic_quality'] = 'excellent'
        elif result.semantic_accuracy >= 0.90:
            analysis['semantic_quality'] = 'good'
        elif result.semantic_accuracy >= 0.80:
            analysis['semantic_quality'] = 'acceptable'
        else:
            analysis['semantic_quality'] = 'poor'
            analysis['recommendations'].append("–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å –∫–∞–Ω–æ–Ω–∏—á–µ—Å–∫–∏—Ö –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π –∞—Ç–∞–∫")
        
        # –û—Ü–µ–Ω–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞
        if hasattr(result, 'timing_analysis'):
            avg_diff = getattr(result.timing_analysis, 'average_difference', 1000)
            if avg_diff < 0.1:
                analysis['timing_quality'] = 'excellent'
            elif avg_diff < 0.5:
                analysis['timing_quality'] = 'good'
            elif avg_diff < 1.0:
                analysis['timing_quality'] = 'acceptable'
            else:
                analysis['timing_quality'] = 'poor'
                analysis['recommendations'].append("–ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—é –≤—Ä–µ–º–µ–Ω–∏ –º–µ–∂–¥—É —Ä–µ–∂–∏–º–∞–º–∏")
        
        # –û–±—â–∞—è –æ—Ü–µ–Ω–∫–∞
        scores = [result.semantic_accuracy, result.truth_consistency_score, result.parity_score]
        avg_score = sum(scores) / len(scores)
        
        if avg_score >= 0.95:
            analysis['overall_quality'] = 'excellent'
        elif avg_score >= 0.85:
            analysis['overall_quality'] = 'good'
        elif avg_score >= 0.70:
            analysis['overall_quality'] = 'acceptable'
        else:
            analysis['overall_quality'] = 'poor'
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        if result.truth_consistency_score < 0.90:
            analysis['recommendations'].append("–ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –ª–æ–≥–æ–≤ –∏ PCAP –¥–∞–Ω–Ω—ã—Ö")
        
        if result.parity_score < 0.90:
            analysis['recommendations'].append("–ò—Å—Å–ª–µ–¥—É–π—Ç–µ —Ä–∞–∑–ª–∏—á–∏—è –≤ –ø–æ–≤–µ–¥–µ–Ω–∏–∏ –º–µ–∂–¥—É —Ä–µ–∂–∏–º–∞–º–∏")
        
        return analysis
    
    def validate_domain_comprehensive(self, domain):
        """–ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è –¥–æ–º–µ–Ω–∞"""
        
        # –ò—â–µ–º —Ñ–∞–π–ª—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        discovery_log = f"logs/{domain}_discovery.log"
        service_log = f"logs/{domain}_service.log"
        discovery_pcap = f"pcap/{domain}_discovery.pcap"
        service_pcap = f"pcap/{domain}_service.pcap"
        
        # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –∏–º–µ–Ω–∞ —Ñ–∞–π–ª–æ–≤
        alt_discovery_log = f"logs/{domain}_quick_discovery.log"
        alt_service_log = f"logs/{domain}_quick_service.log"
        
        if not os.path.exists(discovery_log) and os.path.exists(alt_discovery_log):
            discovery_log = alt_discovery_log
        
        if not os.path.exists(service_log) and os.path.exists(alt_service_log):
            service_log = alt_service_log
        
        return self.validate_parity_with_pcap(
            domain, discovery_log, service_log, discovery_pcap, service_pcap
        )
    
    def batch_validate_domains(self, domains):
        """–ü–∞–∫–µ—Ç–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è –¥–æ–º–µ–Ω–æ–≤"""
        
        print(f"\nüöÄ –ü–ê–ö–ï–¢–ù–ê–Ø –í–ê–õ–ò–î–ê–¶–ò–Ø {len(domains)} –î–û–ú–ï–ù–û–í")
        print("=" * 60)
        
        results = {}
        
        for i, domain in enumerate(domains, 1):
            print(f"\n[{i}/{len(domains)}] –í–∞–ª–∏–¥–∞—Ü–∏—è {domain}")
            print("-" * 40)
            
            result = self.validate_domain_comprehensive(domain)
            results[domain] = result
        
        # –°–≤–æ–¥–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        print(f"\nüìä –°–í–û–î–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê")
        print("=" * 40)
        
        quality_counts = {'excellent': 0, 'good': 0, 'acceptable': 0, 'poor': 0, 'failed': 0}
        
        for domain, result in results.items():
            if result is None:
                quality = 'failed'
                print(f"{domain:20} | FAILED")
            else:
                quality = result['analysis']['overall_quality']
                metrics = result['metrics']
                print(f"{domain:20} | {quality.upper():10} | "
                      f"S:{metrics['semantic_accuracy']:.2f} "
                      f"T:{metrics['truth_consistency']:.2f} "
                      f"P:{metrics['parity_score']:.2f}")
            
            quality_counts[quality] += 1
        
        print(f"\n–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞:")
        for quality, count in quality_counts.items():
            if count > 0:
                percentage = count / len(domains) * 100
                print(f"  {quality.capitalize():10}: {count:2} ({percentage:5.1f}%)")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–≤–æ–¥–Ω—ã–π –æ—Ç—á–µ—Ç
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        summary_file = f"reports/batch_parity_validation_{timestamp}.json"
        
        summary_data = {
            'timestamp': timestamp,
            'domains': domains,
            'results': results,
            'statistics': quality_counts
        }
        
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary_data, f, indent=2, ensure_ascii=False)
        
        print(f"\nüíæ –°–≤–æ–¥–Ω—ã–π –æ—Ç—á–µ—Ç: {summary_file}")
        
        return results

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    
    print("üîç –í–ê–õ–ò–î–ê–¢–û–† –ü–ê–†–ò–¢–ï–¢–ê –ê–¢–ê–ö")
    print("–í–µ—Ä—Å–∏—è: –ü–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å PCAP")
    print("=" * 50)
    
    validator = AttackParityValidator()
    
    if len(sys.argv) > 1:
        # –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –¥–æ–º–µ–Ω–∞
        domain = sys.argv[1]
        result = validator.validate_domain_comprehensive(domain)
        
        if result:
            quality = result['analysis']['overall_quality']
            print(f"\nüéØ –ò–¢–û–ì–û–í–ê–Ø –û–¶–ï–ù–ö–ê: {quality.upper()}")
            
            if result['analysis']['recommendations']:
                print(f"\nüí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:")
                for rec in result['analysis']['recommendations']:
                    print(f"  ‚Ä¢ {rec}")
        else:
            print(f"\n‚ùå –í–∞–ª–∏–¥–∞—Ü–∏—è {domain} –Ω–µ —É–¥–∞–ª–∞—Å—å")
    
    else:
        # –ü–∞–∫–µ—Ç–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è
        domains = ["youtube.com", "nnmclub.to", "googlevideo.com"]
        
        print(f"–ë—É–¥—É—Ç –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã –¥–æ–º–µ–Ω—ã: {', '.join(domains)}")
        print(f"–î–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –¥–æ–º–µ–Ω–∞: python {sys.argv[0]} <domain>")
        
        input("\n–ù–∞–∂–º–∏—Ç–µ Enter –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è –∏–ª–∏ Ctrl+C –¥–ª—è –æ—Ç–º–µ–Ω—ã...")
        
        results = validator.batch_validate_domains(domains)
        
        # –ò—Ç–æ–≥–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞
        successful = sum(1 for r in results.values() if r is not None)
        print(f"\nüéâ –£—Å–ø–µ—à–Ω–æ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ: {successful}/{len(domains)} –¥–æ–º–µ–Ω–æ–≤")

if __name__ == "__main__":
    main()