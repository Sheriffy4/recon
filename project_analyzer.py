#!/usr/bin/env python3
"""
Project Analyzer - –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –ø—Ä–æ–µ–∫—Ç–∞ –¥–ª—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –ø–æ —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥—É.

–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∫–æ–¥ –ø—Ä–æ–µ–∫—Ç–∞ –∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã—Ö –∑–Ω–∞–Ω–∏–π.
"""

import ast
import json
import os
import re
from dataclasses import dataclass, asdict
from pathlib import Path
from typing import Dict, List, Optional, Any, Set, Tuple
from collections import defaultdict
from datetime import datetime
import logging

# –ò–º–ø–æ—Ä—Ç—ã –∏–∑ –Ω–∞—à–µ–π —Å–∏—Å—Ç–µ–º—ã –∑–Ω–∞–Ω–∏–π
try:
    from core.adaptive_refactored.refactoring_knowledge.automation_metadata import (
        get_automation_generator, RefactoringAutomationMetadata
    )
    from core.adaptive_refactored.refactoring_knowledge.decision_trees import (
        CodeMetrics, RefactoringContext
    )
except ImportError:
    print("‚ö†Ô∏è  –ú–æ–¥—É–ª–∏ –∞–Ω–∞–ª–∏–∑–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º —É–ø—Ä–æ—â–µ–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é")


@dataclass
class FileAnalysis:
    """–ê–Ω–∞–ª–∏–∑ –æ—Ç–¥–µ–ª—å–Ω–æ–≥–æ —Ñ–∞–π–ª–∞."""
    filepath: str
    lines_count: int
    classes_count: int
    methods_count: int
    complexity_score: float
    responsibilities_count: int
    dependencies_count: int
    test_coverage: float
    issues: List[str]
    recommendations: List[str]


@dataclass
class ProjectAnalysis:
    """–ê–Ω–∞–ª–∏–∑ –≤—Å–µ–≥–æ –ø—Ä–æ–µ–∫—Ç–∞."""
    total_files: int
    total_lines: int
    large_files: List[str]  # –§–∞–π–ª—ã > 500 —Å—Ç—Ä–æ–∫
    complex_files: List[str]  # –§–∞–π–ª—ã —Å –≤—ã—Å–æ–∫–æ–π —Å–ª–æ–∂–Ω–æ—Å—Ç—å—é
    god_objects: List[str]  # –ö–ª–∞—Å—Å—ã —Å –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—è–º–∏
    refactoring_candidates: List[FileAnalysis]
    overall_recommendations: List[str]
    automation_potential: float


class ProjectAnalyzer:
    """–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –ø—Ä–æ–µ–∫—Ç–∞ –¥–ª—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –ø–æ —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥—É."""
    
    def __init__(self, project_root: str = "."):
        self.project_root = Path(project_root)
        self.knowledge_base = None
        self._load_knowledge_base()
        
    def _load_knowledge_base(self):
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å –±–∞–∑—É –∑–Ω–∞–Ω–∏–π."""
        try:
            generator = get_automation_generator()
            knowledge_file = self.project_root / "knowledge" / "refactoring_automation_metadata.json"
            if knowledge_file.exists():
                self.knowledge_base = generator.load_metadata(str(knowledge_file))
                print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–∞ –±–∞–∑–∞ –∑–Ω–∞–Ω–∏–π: {knowledge_file}")
            else:
                print("‚ö†Ô∏è  –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑")
        except Exception as e:
            print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π: {e}")
            
    def analyze_file(self, filepath: Path) -> FileAnalysis:
        """–ê–Ω–∞–ª–∏–∑ –æ—Ç–¥–µ–ª—å–Ω–æ–≥–æ Python —Ñ–∞–π–ª–∞."""
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                content = f.read()
                
            # –ü–∞—Ä—Å–∏–Ω–≥ AST
            tree = ast.parse(content)
            
            # –ë–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
            lines_count = len(content.splitlines())
            classes = [node for node in ast.walk(tree) if isinstance(node, ast.ClassDef)]
            methods = [node for node in ast.walk(tree) if isinstance(node, ast.FunctionDef)]
            
            # –ê–Ω–∞–ª–∏–∑ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ (—É–ø—Ä–æ—â–µ–Ω–Ω—ã–π)
            complexity_score = self._calculate_complexity(tree)
            
            # –ê–Ω–∞–ª–∏–∑ –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–µ–π (–ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –º–µ—Ç–æ–¥–æ–≤ –≤ –∫–ª–∞—Å—Å–∞—Ö)
            responsibilities_count = self._count_responsibilities(classes)
            
            # –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π (–ø–æ –∏–º–ø–æ—Ä—Ç–∞–º)
            dependencies_count = len([node for node in ast.walk(tree) 
                                    if isinstance(node, (ast.Import, ast.ImportFrom))])
            
            # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º
            issues = []
            recommendations = []
            
            if lines_count > 1000:
                issues.append(f"–ë–æ–ª—å—à–æ–π —Ñ–∞–π–ª: {lines_count} —Å—Ç—Ä–æ–∫")
                recommendations.append("–†–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–æ–¥—É–ª–µ–π")
                
            if complexity_score > 20:
                issues.append(f"–í—ã—Å–æ–∫–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å: {complexity_score}")
                recommendations.append("–ò–∑–≤–ª–µ–∫–∏—Ç–µ –º–µ—Ç–æ–¥—ã –≤ –æ—Ç–¥–µ–ª—å–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã")
                
            if responsibilities_count > 5:
                issues.append(f"–ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏: {responsibilities_count}")
                recommendations.append("–ü—Ä–∏–º–µ–Ω–∏—Ç–µ –ø–∞—Ç—Ç–µ—Ä–Ω Single Responsibility Principle")
                
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ God Object
            for cls in classes:
                class_methods = [node for node in cls.body if isinstance(node, ast.FunctionDef)]
                if len(class_methods) > 15:
                    issues.append(f"God Object: –∫–ª–∞—Å—Å {cls.name} –∏–º–µ–µ—Ç {len(class_methods)} –º–µ—Ç–æ–¥–æ–≤")
                    recommendations.append(f"–†–∞–∑–¥–µ–ª–∏—Ç–µ –∫–ª–∞—Å—Å {cls.name} –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤")
            
            return FileAnalysis(
                filepath=str(filepath.relative_to(self.project_root)),
                lines_count=lines_count,
                classes_count=len(classes),
                methods_count=len(methods),
                complexity_score=complexity_score,
                responsibilities_count=responsibilities_count,
                dependencies_count=dependencies_count,
                test_coverage=0.0,  # –¢—Ä–µ–±—É–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
                issues=issues,
                recommendations=recommendations
            )
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Ñ–∞–π–ª–∞ {filepath}: {e}")
            return FileAnalysis(
                filepath=str(filepath.relative_to(self.project_root)),
                lines_count=0, classes_count=0, methods_count=0,
                complexity_score=0, responsibilities_count=0,
                dependencies_count=0, test_coverage=0.0,
                issues=[f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {e}"], recommendations=[]
            )
            
    def _calculate_complexity(self, tree: ast.AST) -> float:
        """–£–ø—Ä–æ—â–µ–Ω–Ω—ã–π —Ä–∞—Å—á–µ—Ç —Ü–∏–∫–ª–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π —Å–ª–æ–∂–Ω–æ—Å—Ç–∏."""
        complexity = 1  # –ë–∞–∑–æ–≤–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å
        
        for node in ast.walk(tree):
            if isinstance(node, (ast.If, ast.While, ast.For, ast.AsyncFor)):
                complexity += 1
            elif isinstance(node, ast.Try):
                complexity += len(node.handlers)
            elif isinstance(node, (ast.And, ast.Or)):
                complexity += 1
                
        return complexity
        
    def _count_responsibilities(self, classes: List[ast.ClassDef]) -> int:
        """–ü–æ–¥—Å—á–µ—Ç –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–µ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–µ—Ç–æ–¥–æ–≤ –∫–ª–∞—Å—Å–æ–≤."""
        if not classes:
            return 1
            
        max_responsibilities = 0
        for cls in classes:
            methods = [node for node in cls.body if isinstance(node, ast.FunctionDef)]
            # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –º–µ—Ç–æ–¥—ã –ø–æ –ø—Ä–µ—Ñ–∏–∫—Å–∞–º –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–µ–π
            prefixes = set()
            for method in methods:
                if '_' in method.name:
                    prefix = method.name.split('_')[0]
                    prefixes.add(prefix)
                    
            responsibilities = max(len(prefixes), len(methods) // 5)  # –≠–≤—Ä–∏—Å—Ç–∏–∫–∞
            max_responsibilities = max(max_responsibilities, responsibilities)
            
        return max_responsibilities
        
    def analyze_project(self, include_patterns: List[str] = None, 
                       exclude_patterns: List[str] = None) -> ProjectAnalysis:
        """–ê–Ω–∞–ª–∏–∑ –≤—Å–µ–≥–æ –ø—Ä–æ–µ–∫—Ç–∞."""
        
        if include_patterns is None:
            include_patterns = ["**/*.py"]
        if exclude_patterns is None:
            exclude_patterns = ["**/test_*.py", "**/*_test.py", "**/__pycache__/**", 
                              "**/.*", "**/build/**", "**/dist/**"]
            
        print("üîç –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–µ–∫—Ç–∞...")
        
        # –ù–∞–π—Ç–∏ –≤—Å–µ Python —Ñ–∞–π–ª—ã
        python_files = []
        for pattern in include_patterns:
            python_files.extend(self.project_root.glob(pattern))
            
        # –ò—Å–∫–ª—é—á–∏—Ç—å —Ñ–∞–π–ª—ã –ø–æ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º
        filtered_files = []
        for file_path in python_files:
            should_exclude = False
            for exclude_pattern in exclude_patterns:
                if file_path.match(exclude_pattern):
                    should_exclude = True
                    break
            if not should_exclude and file_path.is_file():
                filtered_files.append(file_path)
                
        print(f"üìÅ –ù–∞–π–¥–µ–Ω–æ {len(filtered_files)} Python —Ñ–∞–π–ª–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
        
        # –ê–Ω–∞–ª–∏–∑ –∫–∞–∂–¥–æ–≥–æ —Ñ–∞–π–ª–∞
        file_analyses = []
        large_files = []
        complex_files = []
        god_objects = []
        total_lines = 0
        
        for i, file_path in enumerate(filtered_files):
            if i % 10 == 0:
                print(f"üìä –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ {i}/{len(filtered_files)} —Ñ–∞–π–ª–æ–≤...")
                
            analysis = self.analyze_file(file_path)
            file_analyses.append(analysis)
            total_lines += analysis.lines_count
            
            # –ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
            if analysis.lines_count > 500:
                large_files.append(analysis.filepath)
            if analysis.complexity_score > 15:
                complex_files.append(analysis.filepath)
            if "God Object" in str(analysis.issues):
                god_objects.append(analysis.filepath)
                
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –Ω–∞ —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥
        refactoring_candidates = [
            analysis for analysis in file_analyses
            if len(analysis.issues) > 0 and analysis.lines_count > 100
        ]
        
        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É (–±–æ–ª—å—à–µ –ø—Ä–æ–±–ª–µ–º = –≤—ã—à–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)
        refactoring_candidates.sort(
            key=lambda x: len(x.issues) * x.lines_count, reverse=True
        )
        
        # –û–±—â–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        overall_recommendations = self._generate_overall_recommendations(
            file_analyses, large_files, complex_files, god_objects
        )
        
        # –†–∞—Å—á–µ—Ç –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª–∞ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏
        automation_potential = self._calculate_automation_potential(refactoring_candidates)
        
        return ProjectAnalysis(
            total_files=len(filtered_files),
            total_lines=total_lines,
            large_files=large_files,
            complex_files=complex_files,
            god_objects=god_objects,
            refactoring_candidates=refactoring_candidates[:10],  # –¢–æ–ø 10
            overall_recommendations=overall_recommendations,
            automation_potential=automation_potential
        )
        
    def _generate_overall_recommendations(self, file_analyses: List[FileAnalysis],
                                        large_files: List[str], complex_files: List[str],
                                        god_objects: List[str]) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ–±—â–∏—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –¥–ª—è –ø—Ä–æ–µ–∫—Ç–∞."""
        recommendations = []
        
        if len(large_files) > 5:
            recommendations.append(
                f"üîß –ù–∞–π–¥–µ–Ω–æ {len(large_files)} –±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–æ–≤. "
                "–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –ø—Ä–∏–º–µ–Ω–∏—Ç—å –ø–∞—Ç—Ç–µ—Ä–Ω 'Split Monolithic Configuration'"
            )
            
        if len(complex_files) > 3:
            recommendations.append(
                f"üéØ –ù–∞–π–¥–µ–Ω–æ {len(complex_files)} —Å–ª–æ–∂–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤. "
                "–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –ø—Ä–∏–º–µ–Ω–∏—Ç—å –ø–∞—Ç—Ç–µ—Ä–Ω 'Extract Method to Component Class'"
            )
            
        if len(god_objects) > 0:
            recommendations.append(
                f"‚ö†Ô∏è  –ù–∞–π–¥–µ–Ω–æ {len(god_objects)} God Objects. "
                "–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω–æ –ø—Ä–∏–º–µ–Ω–∏—Ç—å —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º DI –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤"
            )
            
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π
        if self.knowledge_base:
            total_issues = sum(len(analysis.issues) for analysis in file_analyses)
            if total_issues > 20:
                recommendations.append(
                    "üí° –ù–∞ –æ—Å–Ω–æ–≤–µ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π: —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —Å–æ–∑–¥–∞—Ç—å Facade "
                    "–¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ –ø—Ä–∏ –∫—Ä—É–ø–Ω–æ–º —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥–µ"
                )
                
        return recommendations
        
    def _calculate_automation_potential(self, candidates: List[FileAnalysis]) -> float:
        """–†–∞—Å—á–µ—Ç –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª–∞ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥–∞."""
        if not candidates:
            return 0.0
            
        # –ë–∞–∑–æ–≤—ã–π –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–∏–ø–æ–≤ –ø—Ä–æ–±–ª–µ–º
        automation_scores = []
        
        for candidate in candidates:
            score = 0.0
            
            # –ü—Ä–æ–±–ª–µ–º—ã, –∫–æ—Ç–æ—Ä—ã–µ –ª–µ–≥–∫–æ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞—Ç—å
            for issue in candidate.issues:
                if "–ë–æ–ª—å—à–æ–π —Ñ–∞–π–ª" in issue:
                    score += 0.8  # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ —Ñ–∞–π–ª–æ–≤ –ª–µ–≥–∫–æ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞—Ç—å
                elif "–í—ã—Å–æ–∫–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å" in issue:
                    score += 0.6  # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ—Ç–æ–¥–æ–≤ —á–∞—Å—Ç–∏—á–Ω–æ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä—É–µ–º–æ
                elif "God Object" in issue:
                    score += 0.4  # –¢—Ä–µ–±—É–µ—Ç –±–æ–ª—å—à–µ —Ä—É—á–Ω–æ–π —Ä–∞–±–æ—Ç—ã
                elif "–ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏" in issue:
                    score += 0.7  # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–µ–π –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä—É–µ–º–æ
                    
            automation_scores.append(min(score, 1.0))
            
        return sum(automation_scores) / len(automation_scores) if automation_scores else 0.0
        
    def generate_report(self, analysis: ProjectAnalysis, output_file: str = None) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞ –ø–æ –∞–Ω–∞–ª–∏–∑—É."""
        
        report = f"""
# üìä –û—Ç—á–µ—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É –ø—Ä–æ–µ–∫—Ç–∞

**–î–∞—Ç–∞ –∞–Ω–∞–ª–∏–∑–∞**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
**–í—Å–µ–≥–æ —Ñ–∞–π–ª–æ–≤**: {analysis.total_files}
**–í—Å–µ–≥–æ —Å—Ç—Ä–æ–∫ –∫–æ–¥–∞**: {analysis.total_lines:,}
**–ü–æ—Ç–µ–Ω—Ü–∏–∞–ª –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏**: {analysis.automation_potential:.1%}

## üéØ –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ –∫–∞–Ω–¥–∏–¥–∞—Ç—ã –Ω–∞ —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥

"""
        
        for i, candidate in enumerate(analysis.refactoring_candidates, 1):
            report += f"""
### {i}. `{candidate.filepath}`
- **–°—Ç—Ä–æ–∫ –∫–æ–¥–∞**: {candidate.lines_count}
- **–ö–ª–∞—Å—Å–æ–≤**: {candidate.classes_count}
- **–ú–µ—Ç–æ–¥–æ–≤**: {candidate.methods_count}
- **–°–ª–æ–∂–Ω–æ—Å—Ç—å**: {candidate.complexity_score}
- **–ü—Ä–æ–±–ª–µ–º—ã**: {len(candidate.issues)}

**–í—ã—è–≤–ª–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã**:
"""
            for issue in candidate.issues:
                report += f"- ‚ùå {issue}\n"
                
            report += "\n**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏**:\n"
            for rec in candidate.recommendations:
                report += f"- ‚úÖ {rec}\n"
                
        report += f"""

## üìà –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞

- **–ë–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–æ–≤ (>500 —Å—Ç—Ä–æ–∫)**: {len(analysis.large_files)}
- **–°–ª–æ–∂–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤**: {len(analysis.complex_files)}
- **God Objects**: {len(analysis.god_objects)}

## üí° –û–±—â–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏

"""
        
        for rec in analysis.overall_recommendations:
            report += f"- {rec}\n"
            
        if analysis.large_files:
            report += f"""

## üìÅ –ë–æ–ª—å—à–∏–µ —Ñ–∞–π–ª—ã —Ç—Ä–µ–±—É—é—â–∏–µ –≤–Ω–∏–º–∞–Ω–∏—è

"""
            for file_path in analysis.large_files[:5]:  # –¢–æ–ø 5
                report += f"- `{file_path}`\n"
                
        if analysis.god_objects:
            report += f"""

## ‚ö†Ô∏è  God Objects (–∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)

"""
            for file_path in analysis.god_objects:
                report += f"- `{file_path}`\n"
                
        report += f"""

## üöÄ –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏

1. **–ù–∞—á–Ω–∏—Ç–µ —Å God Objects** - –æ–Ω–∏ –∏–º–µ—é—Ç –Ω–∞–∏–±–æ–ª—å—à–∏–π –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª —É–ª—É—á—à–µ–Ω–∏—è
2. **–ü—Ä–∏–º–µ–Ω–∏—Ç–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π**:
   - Extract Method to Component Class
   - Constructor Injection with Interfaces  
   - Split Monolithic Configuration
3. **–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—é** –≥–¥–µ –≤–æ–∑–º–æ–∂–Ω–æ (–ø–æ—Ç–µ–Ω—Ü–∏–∞–ª: {analysis.automation_potential:.1%})
4. **–°–æ–∑–¥–∞–π—Ç–µ —Ç–µ—Å—Ç—ã** –ø–µ—Ä–µ–¥ —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥–æ–º –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏

## üîß –ö–æ–º–∞–Ω–¥—ã –¥–ª—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è

```bash
# –ü–æ–ª—É—á–∏—Ç—å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Ç–∏–ø–∞ –ø—Ä–æ–±–ª–µ–º
python knowledge_manager.py recommend large_monolithic_classes
python knowledge_manager.py recommend god_objects

# –ü—Ä–∏–º–µ–Ω–∏—Ç—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥ (–∫–æ–≥–¥–∞ –±—É–¥–µ—Ç –≥–æ—Ç–æ–≤)
python auto_refactor.py --file path/to/file.py --pattern extract_method_to_component
```

---
*–û—Ç—á–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–¥–∞ –∏ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥–∞*
"""
        
        if output_file:
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write(report)
            print(f"üìÑ –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {output_file}")
            
        return report


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è CLI."""
    import argparse
    
    parser = argparse.ArgumentParser(description="–ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–µ–∫—Ç–∞ –¥–ª—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –ø–æ —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥—É")
    parser.add_argument("--project", "-p", default=".", help="–ü—É—Ç—å –∫ –ø—Ä–æ–µ–∫—Ç—É (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: —Ç–µ–∫—É—â–∞—è –ø–∞–ø–∫–∞)")
    parser.add_argument("--output", "-o", help="–§–∞–π–ª –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ—Ç—á–µ—Ç–∞")
    parser.add_argument("--include", nargs="*", default=["**/*.py"], help="–ü–∞—Ç—Ç–µ—Ä–Ω—ã —Ñ–∞–π–ª–æ–≤ –¥–ª—è –≤–∫–ª—é—á–µ–Ω–∏—è")
    parser.add_argument("--exclude", nargs="*", 
                       default=["**/test_*.py", "**/*_test.py", "**/__pycache__/**"], 
                       help="–ü–∞—Ç—Ç–µ—Ä–Ω—ã —Ñ–∞–π–ª–æ–≤ –¥–ª—è –∏—Å–∫–ª—é—á–µ–Ω–∏—è")
    parser.add_argument("--top", "-t", type=int, default=5, help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–ø –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –¥–ª—è –ø–æ–∫–∞–∑–∞")
    
    args = parser.parse_args()
    
    print(f"üîç –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–µ–∫—Ç–∞: {args.project}")
    
    analyzer = ProjectAnalyzer(args.project)
    analysis = analyzer.analyze_project(args.include, args.exclude)
    
    # –ü–æ–∫–∞–∑–∞—Ç—å –∫—Ä–∞—Ç–∫—É—é —Å–≤–æ–¥–∫—É
    print(f"\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞:")
    print(f"   üìÅ –í—Å–µ–≥–æ —Ñ–∞–π–ª–æ–≤: {analysis.total_files}")
    print(f"   üìù –í—Å–µ–≥–æ —Å—Ç—Ä–æ–∫: {analysis.total_lines:,}")
    print(f"   üéØ –ö–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –Ω–∞ —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥: {len(analysis.refactoring_candidates)}")
    print(f"   ü§ñ –ü–æ—Ç–µ–Ω—Ü–∏–∞–ª –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏: {analysis.automation_potential:.1%}")
    
    if analysis.refactoring_candidates:
        print(f"\nüî• –¢–æ–ø {min(args.top, len(analysis.refactoring_candidates))} –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤:")
        for i, candidate in enumerate(analysis.refactoring_candidates[:args.top], 1):
            print(f"   {i}. {candidate.filepath} ({candidate.lines_count} —Å—Ç—Ä–æ–∫, {len(candidate.issues)} –ø—Ä–æ–±–ª–µ–º)")
            
    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–æ–ª–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞
    report = analyzer.generate_report(analysis, args.output)
    
    if not args.output:
        print("\n" + "="*80)
        print(report)


if __name__ == "__main__":
    main()