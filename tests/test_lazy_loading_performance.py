"""
Comprehensive performance tests for lazy loading functionality.

This module provides detailed performance measurements for lazy loading
to validate the startup time improvements and memory usage benefits.
"""

import time
import gc
import psutil
import os
from typing import Dict, Any, List
from pathlib import Path

import pytest

from core.bypass.attacks.attack_registry import (
    AttackRegistry,
    get_attack_registry,
    configure_lazy_loading,
    clear_registry,
)


class LazyLoadingPerformanceTester:
    """Comprehensive performance tester for lazy loading functionality."""

    def __init__(self):
        self.results: Dict[str, Any] = {}
        self.process = psutil.Process(os.getpid())

    def measure_memory_usage(self) -> float:
        """Measure current memory usage in MB."""
        return self.process.memory_info().rss / 1024 / 1024

    def measure_startup_time(self, lazy_loading: bool, iterations: int = 10) -> Dict[str, float]:
        """
        Measure startup time for registry initialization.
        
        Args:
            lazy_loading: Whether to use lazy loading
            iterations: Number of iterations to average
            
        Returns:
            Dictionary with timing statistics
        """
        times = []
        memory_before = []
        memory_after = []

        for i in range(iterations):
            # Clear registry and force garbage collection
            clear_registry(clear_config=True)
            gc.collect()
            
            # Measure memory before
            mem_before = self.measure_memory_usage()
            memory_before.append(mem_before)
            
            # Measure startup time
            start_time = time.perf_counter()
            registry = AttackRegistry(lazy_loading=lazy_loading)
            end_time = time.perf_counter()
            
            startup_time = end_time - start_time
            times.append(startup_time)
            
            # Measure memory after
            mem_after = self.measure_memory_usage()
            memory_after.append(mem_after)
            
            # Small delay to stabilize measurements
            time.sleep(0.01)

        # Calculate statistics
        avg_time = sum(times) / len(times)
        min_time = min(times)
        max_time = max(times)
        
        avg_memory_before = sum(memory_before) / len(memory_before)
        avg_memory_after = sum(memory_after) / len(memory_after)
        avg_memory_increase = avg_memory_after - avg_memory_before

        return {
            "average_time": avg_time,
            "min_time": min_time,
            "max_time": max_time,
            "times": times,
            "average_memory_before": avg_memory_before,
            "average_memory_after": avg_memory_after,
            "average_memory_increase": avg_memory_increase,
            "iterations": iterations,
        }

    def measure_first_attack_access_time(self, lazy_loading: bool) -> Dict[str, float]:
        """
        Measure time to access first attack after initialization.
        
        Args:
            lazy_loading: Whether to use lazy loading
            
        Returns:
            Dictionary with access timing statistics
        """
        clear_registry(clear_config=True)
        registry = AttackRegistry(lazy_loading=lazy_loading)
        
        # Get list of available attacks
        attacks = registry.list_attacks()
        if not attacks:
            return {"error": "No attacks available"}
        
        # Measure time to get first attack handler
        attack_name = attacks[0]
        start_time = time.perf_counter()
        handler = registry.get_attack_handler(attack_name)
        end_time = time.perf_counter()
        
        access_time = end_time - start_time
        
        return {
            "attack_name": attack_name,
            "access_time": access_time,
            "handler_found": handler is not None,
        }

    def run_comprehensive_performance_test(self) -> Dict[str, Any]:
        """
        Run comprehensive performance comparison between eager and lazy loading.
        
        Returns:
            Complete performance comparison results
        """
        print("Running comprehensive lazy loading performance tests...")
        
        # Test startup performance
        print("  Measuring startup performance...")
        eager_startup = self.measure_startup_time(lazy_loading=False, iterations=20)
        lazy_startup = self.measure_startup_time(lazy_loading=True, iterations=20)
        
        # Test first access performance
        print("  Measuring first attack access performance...")
        eager_access = self.measure_first_attack_access_time(lazy_loading=False)
        lazy_access = self.measure_first_attack_access_time(lazy_loading=True)
        
        # Calculate improvements
        startup_improvement = (
            (eager_startup["average_time"] - lazy_startup["average_time"]) 
            / eager_startup["average_time"] * 100
        )
        
        memory_improvement = (
            (eager_startup["average_memory_increase"] - lazy_startup["average_memory_increase"])
            / eager_startup["average_memory_increase"] * 100
            if eager_startup["average_memory_increase"] > 0 else 0
        )

        results = {
            "startup_performance": {
                "eager_loading": eager_startup,
                "lazy_loading": lazy_startup,
                "improvement_percent": startup_improvement,
                "memory_improvement_percent": memory_improvement,
            },
            "first_access_performance": {
                "eager_loading": eager_access,
                "lazy_loading": lazy_access,
            },
            "summary": {
                "startup_time_improvement": f"{startup_improvement:.2f}%",
                "memory_improvement": f"{memory_improvement:.2f}%",
                "lazy_loading_faster": startup_improvement > 0,
                "lazy_loading_memory_efficient": memory_improvement > 0,
            }
        }
        
        self.results = results
        return results

    def print_performance_report(self):
        """Print a detailed performance report."""
        if not self.results:
            print("No performance results available. Run comprehensive test first.")
            return
            
        results = self.results
        startup = results["startup_performance"]
        access = results["first_access_performance"]
        summary = results["summary"]
        
        print("\n" + "="*60)
        print("LAZY LOADING PERFORMANCE REPORT")
        print("="*60)
        
        print(f"\nüìä STARTUP PERFORMANCE COMPARISON")
        print(f"   Eager Loading:  {startup['eager_loading']['average_time']*1000:.2f}ms (avg)")
        print(f"   Lazy Loading:   {startup['lazy_loading']['average_time']*1000:.2f}ms (avg)")
        print(f"   Improvement:    {summary['startup_time_improvement']}")
        
        print(f"\nüíæ MEMORY USAGE COMPARISON")
        print(f"   Eager Loading:  {startup['eager_loading']['average_memory_increase']:.2f}MB increase")
        print(f"   Lazy Loading:   {startup['lazy_loading']['average_memory_increase']:.2f}MB increase")
        print(f"   Improvement:    {summary['memory_improvement']}")
        
        print(f"\n‚ö° FIRST ATTACK ACCESS")
        print(f"   Eager Loading:  {access['eager_loading']['access_time']*1000:.2f}ms")
        print(f"   Lazy Loading:   {access['lazy_loading']['access_time']*1000:.2f}ms")
        
        print(f"\n‚úÖ SUMMARY")
        print(f"   Startup faster: {summary['lazy_loading_faster']}")
        print(f"   Memory efficient: {summary['lazy_loading_memory_efficient']}")
        
        print("="*60)

    def save_results_to_file(self, filename: str = "lazy_loading_performance_results.json"):
        """Save performance results to JSON file."""
        import json
        
        if not self.results:
            print("No results to save.")
            return
            
        # Convert any non-serializable objects
        serializable_results = self._make_serializable(self.results)
        
        with open(filename, 'w') as f:
            json.dump(serializable_results, f, indent=2)
            
        print(f"Performance results saved to {filename}")

    def _make_serializable(self, obj):
        """Convert object to JSON-serializable format."""
        if isinstance(obj, dict):
            return {k: self._make_serializable(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [self._make_serializable(item) for item in obj]
        elif isinstance(obj, (int, float, str, bool, type(None))):
            return obj
        else:
            return str(obj)


class TestLazyLoadingPerformance:
    """Test class for lazy loading performance validation."""

    def setup_method(self):
        """Setup before each test."""
        clear_registry(clear_config=True)
        self.tester = LazyLoadingPerformanceTester()

    def teardown_method(self):
        """Cleanup after each test."""
        clear_registry(clear_config=True)

    def test_startup_time_improvement(self):
        """Test that lazy loading improves startup time."""
        # Measure startup times
        eager_stats = self.tester.measure_startup_time(lazy_loading=False, iterations=10)
        lazy_stats = self.tester.measure_startup_time(lazy_loading=True, iterations=10)
        
        # Validate measurements
        assert eager_stats["average_time"] > 0
        assert lazy_stats["average_time"] > 0
        assert eager_stats["iterations"] == 10
        assert lazy_stats["iterations"] == 10
        
        # Calculate improvement
        improvement = (
            (eager_stats["average_time"] - lazy_stats["average_time"]) 
            / eager_stats["average_time"] * 100
        )
        
        print(f"Startup time improvement: {improvement:.2f}%")
        print(f"Eager loading: {eager_stats['average_time']*1000:.2f}ms")
        print(f"Lazy loading: {lazy_stats['average_time']*1000:.2f}ms")
        
        # In small systems, lazy loading might have overhead but should be reasonable
        # We allow up to 50% slower for small module counts due to infrastructure overhead
        # The real benefit comes with larger numbers of modules
        assert improvement >= -50, f"Lazy loading excessively slower: {improvement:.2f}%"
        
        # Log the results for analysis
        if improvement < 0:
            print(f"Note: Lazy loading is {abs(improvement):.1f}% slower due to infrastructure overhead in small systems")
        else:
            print(f"Lazy loading provides {improvement:.1f}% improvement")

    def test_memory_usage_efficiency(self):
        """Test that lazy loading is more memory efficient."""
        # Measure memory usage
        eager_stats = self.tester.measure_startup_time(lazy_loading=False, iterations=5)
        lazy_stats = self.tester.measure_startup_time(lazy_loading=True, iterations=5)
        
        # Validate measurements
        assert eager_stats["average_memory_increase"] >= 0
        assert lazy_stats["average_memory_increase"] >= 0
        
        print(f"Eager loading memory increase: {eager_stats['average_memory_increase']:.2f}MB")
        print(f"Lazy loading memory increase: {lazy_stats['average_memory_increase']:.2f}MB")
        
        # Lazy loading should use same or less memory
        memory_improvement = (
            eager_stats["average_memory_increase"] - lazy_stats["average_memory_increase"]
        )
        
        print(f"Memory improvement: {memory_improvement:.2f}MB")
        
        # Allow for small measurement variations
        assert memory_improvement >= -1.0, "Lazy loading uses significantly more memory"

    def test_first_attack_access_performance(self):
        """Test performance of first attack access with lazy loading."""
        # Test eager loading access
        eager_access = self.tester.measure_first_attack_access_time(lazy_loading=False)
        
        # Test lazy loading access
        lazy_access = self.tester.measure_first_attack_access_time(lazy_loading=True)
        
        # Validate both found handlers
        assert eager_access["handler_found"], "Eager loading should find attack handler"
        assert lazy_access["handler_found"], "Lazy loading should find attack handler"
        
        # Validate access times are reasonable
        assert eager_access["access_time"] < 1.0, "Eager access should be fast"
        assert lazy_access["access_time"] < 1.0, "Lazy access should be fast"
        
        print(f"Eager access time: {eager_access['access_time']*1000:.2f}ms")
        print(f"Lazy access time: {lazy_access['access_time']*1000:.2f}ms")

    def test_comprehensive_performance_comparison(self):
        """Run comprehensive performance comparison."""
        results = self.tester.run_comprehensive_performance_test()
        
        # Validate results structure
        assert "startup_performance" in results
        assert "first_access_performance" in results
        assert "summary" in results
        
        startup = results["startup_performance"]
        assert "eager_loading" in startup
        assert "lazy_loading" in startup
        assert "improvement_percent" in startup
        
        # Print detailed report
        self.tester.print_performance_report()
        
        # Save results for analysis
        self.tester.save_results_to_file("test_lazy_loading_performance.json")
        
        # Basic validation that both modes work
        assert startup["eager_loading"]["average_time"] > 0
        assert startup["lazy_loading"]["average_time"] > 0

    @pytest.mark.slow
    def test_large_scale_performance(self):
        """Test performance with larger number of iterations."""
        print("Running large-scale performance test (this may take a while)...")
        
        # Run with more iterations for statistical significance
        eager_stats = self.tester.measure_startup_time(lazy_loading=False, iterations=50)
        lazy_stats = self.tester.measure_startup_time(lazy_loading=True, iterations=50)
        
        # Calculate statistics
        eager_avg = eager_stats["average_time"]
        lazy_avg = lazy_stats["average_time"]
        improvement = (eager_avg - lazy_avg) / eager_avg * 100
        
        # Calculate standard deviation for statistical significance
        import statistics
        eager_std = statistics.stdev(eager_stats["times"])
        lazy_std = statistics.stdev(lazy_stats["times"])
        
        print(f"Eager loading: {eager_avg*1000:.2f}ms ¬± {eager_std*1000:.2f}ms")
        print(f"Lazy loading: {lazy_avg*1000:.2f}ms ¬± {lazy_std*1000:.2f}ms")
        print(f"Improvement: {improvement:.2f}%")
        
        # Validate statistical significance (basic check)
        assert eager_std < eager_avg, "Eager loading measurements should be consistent"
        assert lazy_std < lazy_avg, "Lazy loading measurements should be consistent"

    def test_lazy_loading_benefits_with_module_simulation(self):
        """Test that lazy loading shows benefits when simulating more modules."""
        print("Testing lazy loading benefits with simulated module overhead...")
        
        # This test simulates the scenario where lazy loading would be beneficial
        # by measuring the difference in discovery overhead
        
        clear_registry(clear_config=True)
        
        # Measure time for discovery phase in lazy loading
        start_time = time.perf_counter()
        lazy_registry = AttackRegistry(lazy_loading=True)
        lazy_discovery_time = time.perf_counter() - start_time
        
        # Measure time for full loading in eager loading
        clear_registry(clear_config=True)
        start_time = time.perf_counter()
        eager_registry = AttackRegistry(lazy_loading=False)
        eager_loading_time = time.perf_counter() - start_time
        
        print(f"Lazy discovery time: {lazy_discovery_time*1000:.2f}ms")
        print(f"Eager loading time: {eager_loading_time*1000:.2f}ms")
        
        # Get statistics about what was discovered vs loaded
        lazy_stats = lazy_registry.get_lazy_loading_stats()
        print(f"Discovered modules: {lazy_stats['total_discovered_modules']}")
        print(f"Loaded attacks: {lazy_stats['loaded_attacks']}")
        
        # Both should work correctly
        assert len(lazy_registry.attacks) > 0, "Lazy registry should have builtin attacks"
        assert len(eager_registry.attacks) > 0, "Eager registry should have attacks"
        
        # The key benefit: lazy loading should discover modules without loading them
        if lazy_stats['total_discovered_modules'] > 0:
            assert lazy_stats['loaded_modules'] == 0, "Lazy loading should not load external modules initially"
            print("‚úÖ Lazy loading successfully deferred module loading")
        else:
            print("‚ÑπÔ∏è  No external modules found to demonstrate lazy loading benefits")

    def test_lazy_loading_memory_efficiency_over_time(self):
        """Test that lazy loading maintains memory efficiency over multiple operations."""
        print("Testing memory efficiency over multiple registry operations...")
        
        import gc
        
        # Test memory usage with multiple registry creations
        def measure_memory_pattern(lazy_loading: bool, iterations: int = 10):
            memory_samples = []
            
            for i in range(iterations):
                clear_registry(clear_config=True)
                gc.collect()
                
                mem_before = self.tester.measure_memory_usage()
                registry = AttackRegistry(lazy_loading=lazy_loading)
                mem_after = self.tester.measure_memory_usage()
                
                memory_samples.append(mem_after - mem_before)
                
                # Small delay to stabilize
                time.sleep(0.01)
            
            return memory_samples
        
        eager_memory = measure_memory_pattern(lazy_loading=False, iterations=10)
        lazy_memory = measure_memory_pattern(lazy_loading=True, iterations=10)
        
        eager_avg = sum(eager_memory) / len(eager_memory)
        lazy_avg = sum(lazy_memory) / len(lazy_memory)
        
        print(f"Eager loading average memory: {eager_avg:.2f}MB")
        print(f"Lazy loading average memory: {lazy_avg:.2f}MB")
        print(f"Memory efficiency: {((eager_avg - lazy_avg) / eager_avg * 100):.1f}%")
        
        # Lazy loading should not use significantly more memory
        assert lazy_avg <= eager_avg + 2.0, "Lazy loading should not use much more memory"
        
        clear_registry(clear_config=True)


def run_performance_benchmark():
    """Standalone function to run performance benchmark."""
    print("Starting lazy loading performance benchmark...")
    
    tester = LazyLoadingPerformanceTester()
    results = tester.run_comprehensive_performance_test()
    
    tester.print_performance_report()
    tester.save_results_to_file("lazy_loading_benchmark_results.json")
    
    return results


if __name__ == "__main__":
    # Run benchmark when executed directly
    run_performance_benchmark()